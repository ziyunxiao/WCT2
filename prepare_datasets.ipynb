{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# data_root = \"/data/data-science/rsong/data/805\"\n",
    "# files = os.listdir(f\"{data_root}/style\")\n",
    "# for fname in files:\n",
    "#     print(fname)\n",
    "#     arr = fname.split(\"_\")    \n",
    "#     # nf = fname.replace(\"_rightImg8bit\",\"\")\n",
    "#     nf = f\"{arr[1].zfill(7)}.{fname[-3:]}\"\n",
    "#     os.rename(f\"{data_root}/style/{fname}\",f\"{data_root}/style/{nf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# data_root = \"/data/data-science/rsong/data/805\"\n",
    "# files = os.listdir(f\"{data_root}/style_segment\")\n",
    "# for fname in files:\n",
    "#     print(fname)\n",
    "#     arr = fname.split(\"_\")    \n",
    "#     # nf = fname.replace(\"_rightImg8bit\",\"\")\n",
    "#     nf = f\"{arr[1].zfill(7)}.{fname[-3:]}\"\n",
    "#     os.rename(f\"{data_root}/style_segment/{fname}\",f\"{data_root}/style_segment/{nf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove alpha channel from content and segment png files\n",
    "# cd segment folder then run \n",
    "# for i in `ls *.png`; do convert $i -background black -alpha remove -alpha off $i; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 2048, 3)\n",
      "(540, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "with Image.open(\"./805/style/0000001.png\") as im:\n",
    "    # print(im.size)\n",
    "    arr = np.asarray(im)\n",
    "    print(arr.shape)\n",
    "\n",
    "with Image.open(\"./examples/style/in00.png\") as im:\n",
    "    # print(im.size)\n",
    "    arr = np.array(im)\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]/data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ transfer: ./805/output/0000000.png\n",
      "Elapsed time in whole WCT: 0:00:00.124029\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [3, 3, 1, 1], expected input[1, 4, 512, 896] to have 3 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/data-science/rsong/WCT2/prepare_datasets.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/data/data-science/rsong/WCT2/prepare_datasets.ipynb#ch0000003?line=28'>29</a>\u001b[0m config[\u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/data/data-science/rsong/WCT2/prepare_datasets.ipynb#ch0000003?line=29'>30</a>\u001b[0m config[\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdata_root\u001b[39m}\u001b[39;00m\u001b[39m/output\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/data/data-science/rsong/WCT2/prepare_datasets.ipynb#ch0000003?line=32'>33</a>\u001b[0m run_bulk2(config)\n",
      "File \u001b[0;32m/data/data-science/rsong/WCT2/transfer.py:255\u001b[0m, in \u001b[0;36mrun_bulk2\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=252'>253</a>\u001b[0m         wct2 \u001b[39m=\u001b[39m WCT2(transfer_at\u001b[39m=\u001b[39mtransfer_at, option_unpool\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39moption_unpool\u001b[39m\u001b[39m\"\u001b[39m), device\u001b[39m=\u001b[39mdevice, verbose\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=253'>254</a>\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=254'>255</a>\u001b[0m             img \u001b[39m=\u001b[39m wct2\u001b[39m.\u001b[39;49mtransfer(content, style, content_segment, style_segment, alpha\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39malpha\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=255'>256</a>\u001b[0m         save_image(img\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), fname_output, padding\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=256'>257</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/data-science/rsong/WCT2/transfer.py:98\u001b[0m, in \u001b[0;36mWCT2.transfer\u001b[0;34m(self, content, style, content_segment, style_segment, alpha)\u001b[0m\n\u001b[1;32m     <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=94'>95</a>\u001b[0m wct2_skip_level \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpool1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpool2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpool3\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=96'>97</a>\u001b[0m \u001b[39mfor\u001b[39;00m level \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]:\n\u001b[0;32m---> <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=97'>98</a>\u001b[0m     content_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(content_feat, content_skips, level)\n\u001b[1;32m     <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=98'>99</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransfer_at \u001b[39mand\u001b[39;00m level \u001b[39min\u001b[39;00m wct2_enc_level:\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=99'>100</a>\u001b[0m         content_feat \u001b[39m=\u001b[39m feature_wct(content_feat, style_feats[\u001b[39m'\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m'\u001b[39m][level],\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=100'>101</a>\u001b[0m                                    content_segment, style_segment,\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=101'>102</a>\u001b[0m                                    label_set, label_indicator,\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=102'>103</a>\u001b[0m                                    alpha\u001b[39m=\u001b[39malpha, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/data/data-science/rsong/WCT2/transfer.py:67\u001b[0m, in \u001b[0;36mWCT2.encode\u001b[0;34m(self, x, skips, level)\u001b[0m\n\u001b[1;32m     <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=65'>66</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x, skips, level):\n\u001b[0;32m---> <a href='file:///data/data-science/rsong/WCT2/transfer.py?line=66'>67</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mencode(x, skips, level)\n",
      "File \u001b[0;32m/data/data-science/rsong/WCT2/model.py:163\u001b[0m, in \u001b[0;36mWaveEncoder.encode\u001b[0;34m(self, x, skips, level)\u001b[0m\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/model.py?line=160'>161</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moption_unpool \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcat5\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/model.py?line=161'>162</a>\u001b[0m     \u001b[39mif\u001b[39;00m level \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='file:///data/data-science/rsong/WCT2/model.py?line=162'>163</a>\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv0(x)\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/model.py?line=163'>164</a>\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(out)))\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/model.py?line=164'>165</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///data/data-science/rsong/WCT2/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 3, 1, 1], expected input[1, 4, 512, 896] to have 3 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "# process data\n",
    "from transfer import run_bulk2, get_default_config\n",
    "\n",
    "# config = {\n",
    "#         \"alpha\": 1, \n",
    "#         \"content\": './examples/content', \n",
    "#         'content_segment': None, \n",
    "#         \"cpu\": False, \n",
    "#         \"image_size\": 512, \n",
    "#         \"option_unpool\": 'cat5', \n",
    "#         \"output\": './outputs', \n",
    "#         \"style\": './examples/style', \n",
    "#         \"style_segment\": None, \n",
    "#         \"transfer_all\": False, \n",
    "#         \"transfer_at_decoder\": False, \n",
    "#         \"transfer_at_encoder\": False, \n",
    "#         \"transfer_at_skip\": False, \n",
    "#         \"verbose\": False\n",
    "#     }\n",
    "\n",
    "data_root = \"./805\"\n",
    "config = get_default_config()\n",
    "# config[\"transfer_all\"] = True\n",
    "config[\"transfer_at_skip\"] = True\n",
    "config[\"content\"] = f\"{data_root}/content\"\n",
    "config[\"content_segment\"] = f\"{data_root}/content_segment\"\n",
    "config[\"style\"] = f\"{data_root}/style\"\n",
    "config[\"style_segment\"] = f\"{data_root}/style_segment\"\n",
    "config[\"verbose\"] = True\n",
    "config[\"output\"] = f\"{data_root}/output\"\n",
    "\n",
    "\n",
    "run_bulk2(config)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7f363fa719ab3be540f1d79c70d4eedc7c3ae7f8ebb3a148aa32b37f6d958b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
